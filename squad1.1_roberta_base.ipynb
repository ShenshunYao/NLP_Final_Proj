{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tonyy\\venv\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: requests in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: six in c:\\users\\tonyy\\venv\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\tonyy\\venv\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\tonyy\\venv\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tonyy\\venv\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tonyy\\venv\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonyy\\venv\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in c:\\users\\tonyy\\venv\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: click in c:\\users\\tonyy\\venv\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tonyy\\venv\\lib\\site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: joblib in c:\\users\\tonyy\\venv\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\tonyy\\venv\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "local_dir = './squad_dataset/'\n",
    "\n",
    "# The filenames and URLs for the dataset files.\n",
    "files = [('train-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json'), \n",
    "         ('dev-v1.1.json', 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json'),\n",
    "         ('evaluate-v1.1.py', 'https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py')]\n",
    "\n",
    "# Create the local folder for the files.\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)\n",
    "\n",
    "# Download each of the files.\n",
    "for (filename, url) in files:\n",
    "\n",
    "    # Construct the local file path.\n",
    "    file_path = local_dir + filename\n",
    "\n",
    "    # Download the file (if we haven't already)\n",
    "    if not os.path.exists(file_path):\n",
    "        print('  ' + file_path)\n",
    "        wget.download(url, local_dir + filename)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./squad_dataset/\n",
      "     dev-v1.1.json                  4.63 MB\n",
      "     evaluate-v1.1.py               0.00 MB\n",
      "     train-v1.1.json               28.89 MB\n"
     ]
    }
   ],
   "source": [
    "data_dir = './squad_dataset/'\n",
    "\n",
    "# Check out the sizes on the saved files.\n",
    "files = list(os.listdir(data_dir))\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "# For each file in the directory...\n",
    "for f in files:\n",
    "    # Get the file size, in MB\n",
    "    f_size = float(os.stat(data_dir + '/' + f).st_size) / 2**20\n",
    "    \n",
    "    # Print the filename and its size.\n",
    "    print(\"     {:25s}    {:>6.2f} MB\".format(f, f_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking SQuAD Examples...\n",
      "DONE!\n",
      "There are 87,599 training examples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the training dataset file.\n",
    "with open(os.path.join('./squad_dataset/train-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "print('Unpacking SQuAD Examples...')\n",
    "\n",
    "#print('Articles:')\n",
    "\n",
    "# We'll unpack all of the \n",
    "examples = []\n",
    "\n",
    "# For each Wikipedia article in the dataset...\n",
    "for entry in input_data:\n",
    "\n",
    "    # The Wikipedia Article title.\n",
    "    title = entry[\"title\"]\n",
    "    #print('  ', title)\n",
    "\n",
    "    # For each paragraph in the article...\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "        \n",
    "        # The paragraph, where the answer is found, is referred to as the\n",
    "        # \"context\".\n",
    "        context_text = paragraph[\"context\"]\n",
    "        \n",
    "        # There can be multiple questions per paragraph.\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            \n",
    "            # Define a dictionary to store the properties.\n",
    "            ex = {}\n",
    "\n",
    "            # The unique ID of this question.\n",
    "            ex['qas_id'] = qa[\"id\"]\n",
    "\n",
    "            # The question.\n",
    "            ex['question_text'] = qa[\"question\"]\n",
    "\n",
    "            # In the training data, there is only one answer per question.\n",
    "            answer = qa[\"answers\"][0]\n",
    "\n",
    "            # The answer string.\n",
    "            ex['answer_text'] = answer[\"text\"]\n",
    "\n",
    "            # The character index of the answer in the context.\n",
    "            ex['start_position_character'] = answer[\"answer_start\"]                \n",
    "\n",
    "            # Store the title and paragraph text.\n",
    "            ex['title'] = title\n",
    "            ex['context_text'] = context_text\n",
    "\n",
    "            examples.append(ex)\n",
    "\n",
    "print('DONE!')\n",
    "print('There are {:,} training examples.'.format(len(examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Genocide\n",
      "ID: 5733963c4776f41900660df8\n",
      "\n",
      "======== Question =========\n",
      "What form of destruction was considered too limited by a smaller group of experts?\n",
      "\n",
      "======== Context =========\n",
      "In 2007 the European Court of Human Rights (ECHR), noted in its judgement on\n",
      "Jorgic v. Germany case that in 1992 the majority of legal scholars took the\n",
      "narrow view that \"intent to destroy\" in the CPPCG meant the intended physical-\n",
      "biological destruction of the protected group and that this was still the\n",
      "majority opinion. But the ECHR also noted that a minority took a broader view\n",
      "and did not consider biological-physical destruction was necessary as the intent\n",
      "to destroy a national, racial, religious or ethnic group was enough to qualify\n",
      "as genocide.\n",
      "\n",
      "======== Answer =========\n",
      "biological-physical\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "# Select an example to check out.\n",
    "ex = examples[1200]\n",
    "\n",
    "print('Title:', ex['title'])\n",
    "print('ID:', ex['qas_id'])\n",
    "\n",
    "print('\\n======== Question =========')\n",
    "print(ex['question_text'])\n",
    "\n",
    "print('\\n======== Context =========')\n",
    "print(wrapper.fill(ex['context_text']))\n",
    "\n",
    "print('\\n======== Answer =========')\n",
    "print(ex['answer_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    # Divide the total iterations by the desired number of updates. Most likely\n",
    "    # this will be some ugly number.\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval\n",
    "\n",
    "\n",
    "def check_gpu_mem():\n",
    "    '''\n",
    "    Uses Nvidia's SMI tool to check the current GPU memory usage.\n",
    "    Reported values are in \"MiB\". 1 MiB = 2^20 bytes = 1,048,576 bytes.\n",
    "    '''\n",
    "    \n",
    "    # Run the command line tool and get the results.\n",
    "    buf = os.popen('nvidia-smi --query-gpu=memory.total,memory.used --format=csv')\n",
    "\n",
    "    # Use csv module to read and parse the result.\n",
    "    reader = csv.reader(buf, delimiter=',')\n",
    "\n",
    "    # Use a pandas table just for nice formatting.\n",
    "    df = pd.DataFrame(reader)\n",
    "\n",
    "    # Use the first row as the column headers.\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header #set the header row as the df header\n",
    "\n",
    "    # Display the formatted table.\n",
    "    #display(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "# Set this flag to `True` to load a version of BERT-large which has already been\n",
    "# fine-tuned on SQuAD.\n",
    "pre_tuned = True\n",
    "\n",
    "# If using the pre-fine-tuned BERT-large model...\n",
    "if pre_tuned:\n",
    "\n",
    "    # Load the tokenizer.\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\n",
    "        'csarron/roberta-base-squad-v1',\n",
    "        do_lower_case=True\n",
    "    )\n",
    "\n",
    "    # Create the model and initialize the weights.\n",
    "    model = RobertaForQuestionAnswering.from_pretrained(\n",
    "        'csarron/roberta-base-squad-v1', \n",
    "    )\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    desc = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking SQuAD Examples...\n",
      "Articles:\n",
      "   Super_Bowl_50\n",
      "   Warsaw\n",
      "   Normans\n",
      "   Nikola_Tesla\n",
      "   Computational_complexity_theory\n",
      "   Teacher\n",
      "   Martin_Luther\n",
      "   Southern_California\n",
      "   Sky_(United_Kingdom)\n",
      "   Victoria_(Australia)\n",
      "   Huguenot\n",
      "   Steam_engine\n",
      "   Oxygen\n",
      "   1973_oil_crisis\n",
      "   Apollo_program\n",
      "   European_Union_law\n",
      "   Amazon_rainforest\n",
      "   Ctenophora\n",
      "   Fresno,_California\n",
      "   Packet_switching\n",
      "   Black_Death\n",
      "   Geology\n",
      "   Newcastle_upon_Tyne\n",
      "   Victoria_and_Albert_Museum\n",
      "   American_Broadcasting_Company\n",
      "   Genghis_Khan\n",
      "   Pharmacy\n",
      "   Immune_system\n",
      "   Civil_disobedience\n",
      "   Construction\n",
      "   Private_school\n",
      "   Harvard_University\n",
      "   Jacksonville,_Florida\n",
      "   Economic_inequality\n",
      "   Doctor_Who\n",
      "   University_of_Chicago\n",
      "   Yuan_dynasty\n",
      "   Kenya\n",
      "   Intergovernmental_Panel_on_Climate_Change\n",
      "   Chloroplast\n",
      "   Prime_number\n",
      "   Rhine\n",
      "   Scottish_Parliament\n",
      "   Islamism\n",
      "   Imperialism\n",
      "   United_Methodist_Church\n",
      "   French_and_Indian_War\n",
      "   Force\n",
      "DONE!\n",
      "There are 10,570 test examples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the training dataset file.\n",
    "with open(os.path.join('./squad_dataset/dev-v1.1.json'), \"r\", encoding=\"utf-8\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "\n",
    "print_count = 0\n",
    "\n",
    "print('Unpacking SQuAD Examples...')\n",
    "\n",
    "print('Articles:')\n",
    "\n",
    "# We'll unpack all of the \n",
    "examples = []\n",
    "\n",
    "# For each Wikipedia article in the dataset...\n",
    "for entry in input_data:\n",
    "\n",
    "    # The Wikipedia Article title.\n",
    "    title = entry[\"title\"]\n",
    "    print('  ', title)\n",
    "\n",
    "    # The article contains multiple paragraphs...\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "        \n",
    "        # The paragraph, where the answer is found, is referred to as the\n",
    "        # \"context\".\n",
    "        context_text = paragraph[\"context\"]\n",
    "        \n",
    "        # There can be multiple questions per paragraph.\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            \n",
    "            # Define a dictionary to store the properties.\n",
    "            ex = {}\n",
    "\n",
    "            # The unique ID of this question.\n",
    "            ex['qas_id'] = qa[\"id\"]\n",
    "\n",
    "            # The question.\n",
    "            ex['question_text'] = qa[\"question\"]\n",
    "\n",
    "            # In the test data, there are three answers per question, so we'll\n",
    "            # store all three. \n",
    "            # Each answer has two fields: `answer_start` and `text`.\n",
    "            ex['answers'] = qa[\"answers\"]\n",
    "\n",
    "            # Store the title and paragraph text.\n",
    "            ex['title'] = title\n",
    "            ex['context_text'] = context_text\n",
    "\n",
    "            examples.append(ex)\n",
    "\n",
    "print('DONE!')\n",
    "print('There are {:,} test examples.'.format(len(examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10,570 examples...\n",
      "  Example   1,000  of   10,570.    Elapsed: 0:00:02. Remaining: 0:00:17\n",
      "  Example   2,000  of   10,570.    Elapsed: 0:00:03. Remaining: 0:00:14\n",
      "  Example   3,000  of   10,570.    Elapsed: 0:00:05. Remaining: 0:00:13\n",
      "  Example   4,000  of   10,570.    Elapsed: 0:00:07. Remaining: 0:00:12\n",
      "  Example   5,000  of   10,570.    Elapsed: 0:00:10. Remaining: 0:00:11\n",
      "  Example   6,000  of   10,570.    Elapsed: 0:00:12. Remaining: 0:00:09\n",
      "  Example   7,000  of   10,570.    Elapsed: 0:00:14. Remaining: 0:00:07\n",
      "  Example   8,000  of   10,570.    Elapsed: 0:00:16. Remaining: 0:00:05\n",
      "  Example   9,000  of   10,570.    Elapsed: 0:00:18. Remaining: 0:00:03\n",
      "  Example  10,000  of   10,570.    Elapsed: 0:00:20. Remaining: 0:00:01\n",
      "DONE.  Tokenization took 0:00:21\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# By default, the tokenizer will spit out a warning whenever we tokenize a \n",
    "# sample which ends up being more than 512 tokens. We don't care about that for\n",
    "# now, though, and this cell will produce a lot of those warnings! So we'll \n",
    "# adjust the logging settings to suppress those warnings and keep the output\n",
    "# cell cleaner.\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "# Track the time. Tokenizing all training examples takes around 3 minutes.\n",
    "t0 = time.time()\n",
    "\n",
    "# Lists to store locations\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "# We'll count up the number of answers which are truncated, as well as the\n",
    "# number of test samples for which all three answers were truncated (it's \n",
    "# impossible for us to answer these).\n",
    "num_clipped_answers = 0\n",
    "num_impossible = 0\n",
    "\n",
    "# Pick an interval on which to print progress updates.\n",
    "update_interval = good_update_interval(\n",
    "            total_iters = len(examples), \n",
    "            num_desired_updates = 15\n",
    "        )\n",
    "\n",
    "print('Processing {:,} examples...'.format(len(examples)))\n",
    "\n",
    "# For each of the training examples...\n",
    "for (ex_num, ex) in enumerate(examples):\n",
    "\n",
    "    # =====================\n",
    "    #   Progress Update\n",
    "    # =====================\n",
    "\n",
    "    # Progress update every, e.g., 10k samples.\n",
    "    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n",
    "\n",
    "        # Calculate elapsed time and format it.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        ex_per_sec = (time.time() - t0) / ex_num\n",
    "        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n",
    "\n",
    "    # To store the start and end indeces of the three possible answers.\n",
    "    start_options = []\n",
    "    end_options = []\n",
    "\n",
    "    # Flag to indicate whether we've saved the encoded form of the input yet.\n",
    "    # We'll tokenize the input three times, but only need to store it once!\n",
    "    encoded_stored = False\n",
    "\n",
    "    # For each of the three possible answers...\n",
    "    for answer in ex['answers']:\n",
    "\n",
    "        # =============================\n",
    "        #     Add Sentinel String\n",
    "        # =============================\n",
    "        # To help us determine which of the BERT tokens correspond to the answer,\n",
    "        # we'll replace the answer with, e.g., \"[MASK] [MASK] [MASK]\" (based on \n",
    "        # the number of tokens in the answer).\n",
    "\n",
    "        # Tokenize the answer--it may be broken into multiple words and/or subwords.\n",
    "        answer_tokens = tokenizer.tokenize(answer['text'])\n",
    "\n",
    "        # Create our sentinel string, e.g., \"[MASK] [MASK] [MASK]\"\n",
    "        sentinel_str = ' '.join(['<mask>']*len(answer_tokens))\n",
    "\n",
    "        # Within the \"context\" string, replace the answer with our sentinel.\n",
    "        # Python doesn't appear to have a built-in function for replacing a \n",
    "        # substring *starting at a specific index*, so we'll implement it in a \n",
    "        # more manual way.\n",
    "\n",
    "        # Locate the exact start and end of the answer text within the \"context\"\n",
    "        # string. The dataset gives us this information because the answer text\n",
    "        # may occur more than once in the context!\n",
    "        start_char_i = answer['answer_start']\n",
    "        end_char_i = start_char_i + len(answer['text'])\n",
    "\n",
    "        # To make the replacement, we use slicing and string concatenation.\n",
    "        context_w_sentinel = ex['context_text'][:start_char_i] + \\\n",
    "                            sentinel_str + \\\n",
    "                            ex['context_text'][end_char_i:]\n",
    "\n",
    "        # =============================\n",
    "        #      Tokenize & Encode\n",
    "        # =============================\n",
    "        # Combine the question and the context strings and encode them.\n",
    "        input_ids = tokenizer.encode(\n",
    "            ex['question_text'], \n",
    "            context_w_sentinel,\n",
    "            add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "            #max_length = max_len,       # Pad & truncate all sentences.\n",
    "            pad_to_max_length = False,\n",
    "            truncation = False,\n",
    "        )\n",
    "\n",
    "        # =============================\n",
    "        #     Locate Answer Tokens\n",
    "        # =============================\n",
    "        # Locate all of the instances of the '[MASK]' token. \n",
    "        \n",
    "        # Find all indeces of the [MASK] token.\n",
    "        mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n",
    "\n",
    "        # Note: You can use the alternate code below if the input_ids are in a \n",
    "        #       PyTorch tensor\n",
    "        # First, compare all of the tokens to the mask token. \n",
    "        #is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n",
    "        # Then get the indeces of the '1's using the `nonzero` function.\n",
    "        #mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\n",
    "\n",
    "        # As a sanity check, make sure the number of MASK tokens we found is the\n",
    "        # same as the number of answer tokens.\n",
    "        assert(len(mask_token_indeces) == len(answer_tokens))           \n",
    "\n",
    "        # `mask_token_indeces` is the range of indeces (e.g., [68, 69, 70, 71]), \n",
    "        # but we really just want the start and end indeces (e.g., 68 and 71).\n",
    "        start_index = mask_token_indeces[0]\n",
    "        end_index = mask_token_indeces[-1]\n",
    "\n",
    "        # Store these indeces in our lists.\n",
    "        start_options.append(start_index)\n",
    "        end_options.append(end_index)\n",
    "    \n",
    "    # Store the start and end indeces of the three possible correct answers.\n",
    "    start_positions.append(start_options)\n",
    "    end_positions.append(end_options)\n",
    "    \n",
    "    # Continue looping through all of the test samples.\n",
    "\n",
    "# =========================\n",
    "#        Wrap-Up\n",
    "# =========================\n",
    "\n",
    "print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 10,570 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tonyy\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example   1,000  of   10,570.    Elapsed: 0:00:01. Remaining: 0:00:05\n",
      "  Example   2,000  of   10,570.    Elapsed: 0:00:01. Remaining: 0:00:04\n",
      "  Example   3,000  of   10,570.    Elapsed: 0:00:02. Remaining: 0:00:04\n",
      "  Example   4,000  of   10,570.    Elapsed: 0:00:02. Remaining: 0:00:03\n",
      "  Example   5,000  of   10,570.    Elapsed: 0:00:03. Remaining: 0:00:03\n",
      "  Example   6,000  of   10,570.    Elapsed: 0:00:03. Remaining: 0:00:03\n",
      "  Example   7,000  of   10,570.    Elapsed: 0:00:04. Remaining: 0:00:02\n",
      "  Example   8,000  of   10,570.    Elapsed: 0:00:05. Remaining: 0:00:01\n",
      "  Example   9,000  of   10,570.    Elapsed: 0:00:05. Remaining: 0:00:01\n",
      "  Example  10,000  of   10,570.    Elapsed: 0:00:06. Remaining: 0:00:00\n",
      "DONE.  Tokenization took 0:00:06\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Track the time. Tokenizing all training examples takes around 3 minutes.\n",
    "t0 = time.time()\n",
    "\n",
    "# Lists to store the encoded samples.\n",
    "all_input_ids = []\n",
    "attention_masks = []\n",
    "segment_ids = [] \n",
    "\n",
    "# Pick an interval on which to print progress updates.\n",
    "update_interval = good_update_interval(\n",
    "            total_iters = len(examples), \n",
    "            num_desired_updates = 15\n",
    "        )\n",
    "\n",
    "print('Tokenizing {:,} examples...'.format(len(examples)))\n",
    "\n",
    "# For each of the training examples...\n",
    "for (ex_num, ex) in enumerate(examples):\n",
    "\n",
    "    # =====================\n",
    "    #   Progress Update\n",
    "    # =====================\n",
    "\n",
    "    # Progress update every, e.g., 10k samples.\n",
    "    if (ex_num % update_interval) == 0 and not (ex_num == 0):\n",
    "\n",
    "        # Calculate elapsed time and format it.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        ex_per_sec = (time.time() - t0) / ex_num\n",
    "        remaining_sec = ex_per_sec * (len(examples) - ex_num)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Example {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(ex_num, len(examples), elapsed, remaining))\n",
    "\n",
    "    # =============================\n",
    "    #      Tokenize & Encode\n",
    "    # =============================\n",
    "    # Combine the question and the context strings, and tokenize them all \n",
    "    # together.\n",
    "    # `encode_plus` will:    \n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Place an `[SEP]` token between the question and reference text, and \n",
    "    #       and at the end of the reference text.\n",
    "    #   (4) Map tokens to their IDs (\"encode\" the text)\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    #   (7) Create the list of segment IDs, indicating which tokens belong\n",
    "    #       to the question vs. the context.\n",
    "    #   (8) Casts everything as PyTorch tensors.\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        ex['question_text'], \n",
    "        ex['context_text'],\n",
    "        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length = max_len,       # Pad & truncate all sentences.\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True,\n",
    "        return_attention_mask = True, # Construct attention masks.\n",
    "        return_tensors = 'pt',        # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Retrieve the encoded sequence.\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "\n",
    "    # =============================\n",
    "    #     Store Encoded Sample\n",
    "    # =============================\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    all_input_ids.append(input_ids)\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])    \n",
    "\n",
    "    # ^^^ Continue looping through all of the test samples. ^^^\n",
    "\n",
    "# =========================\n",
    "#        Wrap-Up\n",
    "# =========================\n",
    "\n",
    "# Convert the lists of tensors into 2D tensors.\n",
    "all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# We don't need the indeces to be tensors, since we're not doing training here.\n",
    "# Convert the \"labels\" (the start and end indeces) into tensors.\n",
    "#start_positions = torch.tensor(start_positions)\n",
    "#end_positions = torch.tensor(end_positions)\n",
    "\n",
    "print('DONE.  Tokenization took {:}'.format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 661 test batches...\n",
      "  Batch      50  of      661.    Elapsed: 0:00:07. Remaining: 0:01:20\n",
      "  Batch     100  of      661.    Elapsed: 0:00:13. Remaining: 0:01:12\n",
      "  Batch     150  of      661.    Elapsed: 0:00:19. Remaining: 0:01:05\n",
      "  Batch     200  of      661.    Elapsed: 0:00:25. Remaining: 0:00:58\n",
      "  Batch     250  of      661.    Elapsed: 0:00:32. Remaining: 0:00:52\n",
      "  Batch     300  of      661.    Elapsed: 0:00:38. Remaining: 0:00:46\n",
      "  Batch     350  of      661.    Elapsed: 0:00:44. Remaining: 0:00:39\n",
      "  Batch     400  of      661.    Elapsed: 0:00:51. Remaining: 0:00:33\n",
      "  Batch     450  of      661.    Elapsed: 0:00:57. Remaining: 0:00:27\n",
      "  Batch     500  of      661.    Elapsed: 0:01:03. Remaining: 0:00:20\n",
      "  Batch     550  of      661.    Elapsed: 0:01:10. Remaining: 0:00:14\n",
      "  Batch     600  of      661.    Elapsed: 0:01:16. Remaining: 0:00:08\n",
      "  Batch     650  of      661.    Elapsed: 0:01:22. Remaining: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Tracking variables \n",
    "pred_start = []\n",
    "pred_end = []\n",
    "\n",
    "# Get the total number of test samples (not answers).\n",
    "num_test_samples = all_input_ids.shape[0]\n",
    "\n",
    "# We'll batch the samples to speed up processing. \n",
    "batch_size = 16\n",
    "\n",
    "num_batches = int(np.ceil(num_test_samples / batch_size))\n",
    "\n",
    "print('Training on {:,} test batches...'.format(num_batches))\n",
    "\n",
    "batch_num = 0\n",
    "\n",
    "# Train\n",
    "for start_i in range(0, num_test_samples, batch_size):\n",
    "    # Report progress.\n",
    "    if ((batch_num % 50) == 0) and not (batch_num == 0):\n",
    "\n",
    "        # Calculate elapsed time and format it.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        batches_per_sec = (time.time() - t0) / batch_num\n",
    "        remaining_sec = batches_per_sec * (num_batches - batch_num)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(batch_num, num_batches, elapsed, remaining))\n",
    "\n",
    "    # Calculate the ending index for this batch.\n",
    "    # `end_i` is equal to the index of the last sample in the batch, +1.\n",
    "    end_i = min(start_i + batch_size, num_test_samples)\n",
    "\n",
    "    # Select our batch inputs (`b` stands for batch here).\n",
    "    b_input_ids = all_input_ids[start_i:end_i, :]\n",
    "    b_attn_masks = attention_masks[start_i:end_i, :]\n",
    "\n",
    "    # Copy these to the GPU.\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_attn_masks = b_attn_masks.to(device)\n",
    "    \n",
    "    # Telling the model not to compute or store the compute graph, saving memory \n",
    "    # and speeding up prediction\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, attention_mask=b_attn_masks)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    start_logits = outputs.start_logits.detach().cpu().numpy()\n",
    "    end_logits = outputs.end_logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = np.argmax(start_logits, axis=1)\n",
    "    answer_end = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    pred_start.append(answer_start)\n",
    "    pred_end.append(answer_end)\n",
    "\n",
    "    batch_num += 1\n",
    "\n",
    "    # ^^^ Continue looping through the batches. ^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 661 test batches...\n",
      "  Batch      50  of      661.    Elapsed: 0:00:06. Remaining: 0:01:19\n",
      "  Batch     100  of      661.    Elapsed: 0:00:13. Remaining: 0:01:12\n",
      "  Batch     150  of      661.    Elapsed: 0:00:19. Remaining: 0:01:05\n",
      "  Batch     200  of      661.    Elapsed: 0:00:26. Remaining: 0:00:59\n",
      "  Batch     250  of      661.    Elapsed: 0:00:32. Remaining: 0:00:52\n",
      "  Batch     300  of      661.    Elapsed: 0:00:38. Remaining: 0:00:46\n",
      "  Batch     350  of      661.    Elapsed: 0:00:45. Remaining: 0:00:40\n",
      "  Batch     400  of      661.    Elapsed: 0:00:51. Remaining: 0:00:33\n",
      "  Batch     450  of      661.    Elapsed: 0:00:58. Remaining: 0:00:27\n",
      "  Batch     500  of      661.    Elapsed: 0:01:04. Remaining: 0:00:21\n",
      "  Batch     550  of      661.    Elapsed: 0:01:10. Remaining: 0:00:14\n",
      "  Batch     600  of      661.    Elapsed: 0:01:17. Remaining: 0:00:08\n",
      "  Batch     650  of      661.    Elapsed: 0:01:23. Remaining: 0:00:01\n",
      "    DONE.\n",
      "\n",
      "Evaluation took 85 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Tracking variables \n",
    "pred_start = []\n",
    "pred_end = []\n",
    "\n",
    "# Get the total number of test samples (not answers).\n",
    "num_test_samples = all_input_ids.shape[0]\n",
    "\n",
    "# We'll batch the samples to speed up processing. \n",
    "batch_size = 16\n",
    "\n",
    "num_batches = int(np.ceil(num_test_samples / batch_size))\n",
    "\n",
    "print('Evaluating on {:,} test batches...'.format(num_batches))\n",
    "\n",
    "batch_num = 0\n",
    "\n",
    "# Predict \n",
    "for start_i in range(0, num_test_samples, batch_size):\n",
    "    \n",
    "    # Report progress.\n",
    "    if ((batch_num % 50) == 0) and not (batch_num == 0):\n",
    "\n",
    "        # Calculate elapsed time and format it.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Calculate the time remaining based on our progress.\n",
    "        batches_per_sec = (time.time() - t0) / batch_num\n",
    "        remaining_sec = batches_per_sec * (num_batches - batch_num)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        # Report progress.\n",
    "        print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}. Remaining: {:}'.format(batch_num, num_batches, elapsed, remaining))\n",
    "\n",
    "    # Calculate the ending index for this batch.\n",
    "    # `end_i` is equal to the index of the last sample in the batch, +1.\n",
    "    end_i = min(start_i + batch_size, num_test_samples)\n",
    "\n",
    "    # Select our batch inputs (`b` stands for batch here).\n",
    "    b_input_ids = all_input_ids[start_i:end_i, :]\n",
    "    b_attn_masks = attention_masks[start_i:end_i, :]\n",
    "\n",
    "    # Copy these to the GPU.\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_attn_masks = b_attn_masks.to(device)\n",
    "    \n",
    "    # Telling the model not to compute or store the compute graph, saving memory \n",
    "    # and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, attention_mask=b_attn_masks)\n",
    "                        \n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    start_logits = outputs.start_logits.detach().cpu().numpy()\n",
    "    end_logits = outputs.end_logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = np.argmax(start_logits, axis=1)\n",
    "    answer_end = np.argmax(end_logits, axis=1)\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    pred_start.append(answer_start)\n",
    "    pred_end.append(answer_end)\n",
    "\n",
    "    batch_num += 1\n",
    "\n",
    "    # ^^^ Continue looping through the batches. ^^^\n",
    "\n",
    "# Combine the results across the batches.\n",
    "pred_start = np.concatenate(pred_start, axis=0)\n",
    "pred_end = np.concatenate(pred_end, axis=0)\n",
    "\n",
    "print('    DONE.')\n",
    "\n",
    "print('\\nEvaluation took {:.0f} seconds.'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted indices: 14,348 of 21,140 (67.87%)\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "\n",
    "# For each test sample...\n",
    "for i in range(0, len(pred_start)):\n",
    "\n",
    "    match_options = []\n",
    "\n",
    "    # For each of the three possible answers...\n",
    "    for j in range (0, len(start_positions[i])):\n",
    "    \n",
    "        matches = 0\n",
    "\n",
    "        # Add a point if the start indeces match.\n",
    "        if pred_start[i] == start_positions[i][j]:\n",
    "            matches += 1\n",
    "\n",
    "        # Add a point if the end indeces match.\n",
    "        if pred_end[i] == end_positions[i][j]:\n",
    "            matches += 1\n",
    "\n",
    "        # Store the total.\n",
    "        match_options.append(matches)\n",
    "\n",
    "    # Between the three possible answers, pick the one with the highest \"score\".\n",
    "    total_correct += (max(match_options))\n",
    "\n",
    "    # ^^^ Continue looping through test samples ^^^\n",
    "\n",
    "total_indeces = len(pred_start) + len(pred_end)\n",
    "\n",
    "print('Correctly predicted indices: {:,} of {:,} ({:.2%})'.format(\n",
    "    total_correct,\n",
    "    total_indeces,\n",
    "    float(total_correct) / float(total_indeces)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.800\n"
     ]
    }
   ],
   "source": [
    "# The final F1 score for each sample.\n",
    "f1s = []\n",
    "\n",
    "# For each test sample...\n",
    "for i in range(0, len(pred_start)):\n",
    "\n",
    "    # Expand the start and end indeces into sequences of indeces stored as sets.\n",
    "    # For example, if pred_start = 137 and pred_end = 140, then\n",
    "    #   pred_span = {137, 138, 139, 140}\n",
    "    pred_span = set(range(pred_start[i], pred_end[i] + 1))\n",
    "\n",
    "\n",
    "    f1_options = []\n",
    "\n",
    "    # For each of the three possible answers...\n",
    "    for j in range (0, len(start_positions[i])):\n",
    "    \n",
    "        # Expand this answer into a range, as above.\n",
    "        true_span = set(range(start_positions[i][j], end_positions[i][j] + 1))    \n",
    "\n",
    "        # Use the `intersection` function from Python `set` to get the set of \n",
    "        # indeces occurring in both spans. Take the length of this resulting set\n",
    "        # as the number of overlapping indeces between the two spans.\n",
    "        num_same = len(pred_span.intersection(true_span))    \n",
    "\n",
    "        # If there's no overlap, then the F1 score is 0 for this sample.\n",
    "        if num_same == 0:\n",
    "            f1_options.append(0)\n",
    "            continue\n",
    "\n",
    "        # Precision - How many tokens overlap relative to the total number of tokens\n",
    "        #             in the predicted span? If the model predicts too large of a \n",
    "        #             span, it has bad precision.      \n",
    "        precision = float(num_same) / float(len(pred_span))\n",
    "    \n",
    "        # Recall - How many of the correct tokens made it into the predicted span?\n",
    "        #          A model could have perfect recall if it just predicted the entire\n",
    "        #          paragraph as the answer :).    \n",
    "        recall = float(num_same) / float(len(true_span))\n",
    "\n",
    "        # F1 - Does the model have both good precision and good recall?\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        # Store the score.\n",
    "        f1_options.append(f1)\n",
    "\n",
    "        # ^^^ Continue looping through possible answers ^^^\n",
    "\n",
    "    # Take the highest of the three F1 scores as our score for this sample.\n",
    "    f1s.append(max(f1_options))\n",
    "\n",
    "    # ^^^ Continue looping through test samples ^^^\n",
    "\n",
    "\n",
    "print('Average F1 Score: {:.3f}'.format(np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
